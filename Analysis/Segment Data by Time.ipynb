{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import NavigableString\n",
    "import pandas as pd\n",
    "import StringIO\n",
    "import urllib\n",
    "from datetime import date, datetime, timedelta\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.patches as mpatches\n",
    "from scipy.stats.stats import pearsonr, describe\n",
    "import math\n",
    "import sys\n",
    "from sentiment import Classifier\n",
    "import pytz\n",
    "import ggplot\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "import pytz\n",
    "from dateutil import parser\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 't' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-49733d809d55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/SamuelGreen/Dropbox/__JuniorSpring/AM221/project/Analysis/sentiment.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0mtrained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentiment_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrained\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/SamuelGreen/Dropbox/__JuniorSpring/AM221/project/Analysis/sentiment.py\u001b[0m in \u001b[0;36msentiment_classifier\u001b[0;34m(sensitivity)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Tweet %d of %d\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_tweets\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0mcleantweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_to_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TweetText'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# vectorizer = CountVectorizer(analyzer = \"word\",   \\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/SamuelGreen/Dropbox/__JuniorSpring/AM221/project/Analysis/sentiment.py\u001b[0m in \u001b[0;36mtweet_to_words\u001b[0;34m(tweet)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mfset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmeaningful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0mfset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'contains(%s)'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 't' is not defined"
     ]
    }
   ],
   "source": [
    "classifier = Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Simple lambda function\n",
    "def split(x, num):\n",
    "    return int(x.split('-')[num])\n",
    "\n",
    "# Write a function that scrapes the team trend graph for each game\n",
    "def ncaatrendgraph(gameid):\n",
    "\n",
    "    # Scrape the reference team page\n",
    "    url = 'http://espn.go.com/mens-college-basketball/playbyplay?gameId=' + str(gameid)\n",
    "    html = urllib.urlopen(url)\n",
    "    soup = BeautifulSoup(html, \"html.parser\") \n",
    "\n",
    "    # Identify which team is which from the basic page\n",
    "    teams = soup.find_all('div', {'class', \"team-container\"})\n",
    "\n",
    "    away = str(teams[0].find('span', {'class', 'long-name'}).contents[0]).split(';')[0]\n",
    "    home = str(teams[1].find('span', {'class', 'long-name'}).contents[0]).split(';')[0]\n",
    "\n",
    "    # # Run through the table and get all the relevant events from that quarter\n",
    "    qes = []\n",
    "    tps = len(soup.find_all('table'))-1\n",
    "    for num in range(1,tps):\n",
    "        for item in soup.find_all('table')[num].find_all('tr')[1:]:\n",
    "            event = []\n",
    "            for td in item.find_all('td'):\n",
    "                if len(td.contents) > 0:\n",
    "                    if 'img' in str(td.contents[0]):\n",
    "                        event.append(str(td.contents[0]['src']).split('/')[-1].split('.')[0].upper())\n",
    "                    else:\n",
    "                        if ':' in str(td.contents[0]):\n",
    "                            if num > 2:\n",
    "                                minutes = 4-int(td.contents[0].split(':')[0]) + 40 + ((num-3)*5)\n",
    "                            else:\n",
    "                                minutes = 19-int(td.contents[0].split(':')[0]) +((num-1)*20)\n",
    "                            seconds = 60-int(td.contents[0].split(':')[1])\n",
    "\n",
    "                            # Make an adjustment for exact minute calculations\n",
    "                            if seconds == 60:\n",
    "                                minutes = minutes + 1\n",
    "                                seconds = 0\n",
    "\n",
    "                            event.append(minutes)\n",
    "                            event.append(seconds)\n",
    "                        else:\n",
    "                            event.append(str(td.contents[0]))\n",
    "            qes.append(event)\n",
    "\n",
    "    # Make this data into a Dataframe\n",
    "    bsd = pd.DataFrame(qes, columns = ['Minutes', 'Seconds', 'Team', 'Event', 'Score'])\n",
    "    bsd[away] = bsd['Score'].apply(lambda x: split(x, 0))\n",
    "    bsd[home] = bsd['Score'].apply(lambda x: split(x, 1))\n",
    "    bsd = bsd.drop('Score', 1)\n",
    "\n",
    "    # # Write a quick function converting the minutes and seconds to a percentage\n",
    "    if (tps) == 3:\n",
    "        numminutes = 40\n",
    "    else:\n",
    "        numminutes = 40 + 5*(tps-3)\n",
    "    lengame = numminutes*60.0\n",
    "    bsd['PercDone'] = [float(100*round((mins*60+sec)/lengame,4)) for (mins, sec) in zip(bsd['Minutes'], bsd['Seconds'])]\n",
    "    return bsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = ncaatrendgraph(400873156)\n",
    "g1 = ncaatrendgraph(400873157)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write a function that processes the events that happen during this period\n",
    "\"\"\"\n",
    "Gamedata: The dataframe containing all the data above\n",
    "Start: The time at which you'd like to start looking (%H:%M, ex. 7:20 (EST))\n",
    "Finish: The time at which you'd like to stop looking (see above)\n",
    "\"\"\"\n",
    "def gamevents(gamedata, start, finish, begin, end):\n",
    "    start_conv = datetime.strptime(start, '%H:%M')\n",
    "    finish_conv = datetime.strptime(finish, '%H:%M')\n",
    "        \n",
    "    # Make quick sanity checks\n",
    "    if start_conv < begin:\n",
    "        start_conv = begin\n",
    "    if finish_conv > end:\n",
    "        finish_conv = end\n",
    "    # Check where each event is in relation to halftime\n",
    "    sloc, endloc = 0, 0\n",
    "    lenhalf = 20*60\n",
    "    htbegin = begin + timedelta(0, (end-begin).seconds/2 - lenhalf/2)\n",
    "    htend = begin + timedelta(0,(end-begin).seconds/2 + lenhalf/2)\n",
    "    if start_conv > htend:\n",
    "        sloc = 1\n",
    "    if finish_conv > htend:\n",
    "        endloc = 1\n",
    "        \n",
    "    # Assume for now that both are on the same side of halftime\n",
    "    if endloc == 0:\n",
    "        ttb1, ttb2 = (start_conv - begin).seconds, (finish_conv - begin).seconds\n",
    "        perc1, perc2 = round(100*(1.0*ttb1)/(2*(htbegin-begin).seconds),2), round(100*(1.0*ttb2)/(2*(htbegin-begin).seconds),2)\n",
    "    elif endloc == 1:\n",
    "        ttb1, ttb2 = (start_conv - htend).seconds, (finish_conv - htend).seconds\n",
    "        perc1, perc2 = round(100*(1.0*ttb1)/(end-htend).seconds,2)+50, round(100*(1.0*ttb2)/(end-htend).seconds,2)+50\n",
    "    \n",
    "    # Grab the data from the gamedata\n",
    "    return gamedata[(gamedata['PercDone'] > perc1) & (gamedata['PercDone'] < perc2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Large function that will return to you the relevant game events and then a list of tweets (with their times) \n",
    "from the time period that you specify. Currently works by submitting actual times.\n",
    "\n",
    "Inputs:\n",
    "gp1: The first game participant\n",
    "gp2: The second game participant\n",
    "tp1: Beginning of time period (%H:%M, ex. 7:20), measured by EST\n",
    "tp2: End of time period\n",
    "\n",
    "Outputs:\n",
    "dt1: Dataframe containing all the game events from that period \n",
    "dt2: Dataframe containing the tweets from that period\n",
    "\"\"\"\n",
    "\n",
    "def databytime(gp1, gp2, tp1, tp2):\n",
    "    \n",
    "    # Get the important metadata and define a useful constant\n",
    "    gmdat = pd.read_csv(\"GameMetadata.csv\")\n",
    "    metadat = gmdat[((gmdat['Team1'] == gp1) | (gmdat['Team1'] == gp2)) & ((gmdat['Team2'] == gp1) | (gmdat['Team2'] == gp2))]\n",
    "    eid = metadat['espn_id'].iloc[0]\n",
    "    begin, end = datetime.strptime(metadat['Start'].iloc[0], '%H:%M'), datetime.strptime(metadat['End'].iloc[0], '%H:%M')\n",
    "    lenhalf = 20*60\n",
    "    \n",
    "    # Grab the game data for the time period\n",
    "    gdata = ncaatrendgraph(eid)\n",
    "    dt1 = gamevents(gdata, tp1, tp2, begin, end)\n",
    "    \n",
    "    # Grab the relevant data from the twitter file\n",
    "    fname = '../separated/' + metadat['Filename'].iloc[0]\n",
    "    tweets =  pd.read_csv(fname)\n",
    "    tweets['time_chg'] = tweets['time'].apply(lambda x: x.split(' ')[3])\n",
    "    \n",
    "    indices = []\n",
    "    for num in range(0, len(tweets['time_chg'])):\n",
    "        tweetstamp = datetime.strptime(tweets['time_chg'].iloc[num], \"%H:%M:%S\") - timedelta(hours=4)\n",
    "        if (tweetstamp > begin) and (tweetstamp < end):\n",
    "            print True\n",
    "            if (tweetstamp > datetime.strptime(tp1, '%H:%M')) and (tweetstamp < datetime.strptime(tp2, '%H:%M')):\n",
    "                indices.append(num)\n",
    "    dt2 = tweets[tweets.index.isin(indices)]\n",
    "    \n",
    "    return dt1, dt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getgamemeta(gp1, gp2):\n",
    "    gmdat = pd.read_csv(\"GameMetadata.csv\")\n",
    "    metadat = gmdat[((gmdat['Team1'] == gp1) | (gmdat['Team1'] == gp2)) & ((gmdat['Team2'] == gp1) | (gmdat['Team2'] == gp2))]\n",
    "    eid = metadat['espn_id'].iloc[0]\n",
    "    begin, end = datetime.strptime(metadat['Start'].iloc[0], '%H:%M'), datetime.strptime(metadat['End'].iloc[0], '%H:%M')\n",
    "    lenhalf = 20*60\n",
    "    htbegin = begin + timedelta(0, (end-begin).seconds/2 - lenhalf/2)\n",
    "    htend = begin + timedelta(0,(end-begin).seconds/2 + lenhalf/2)\n",
    "    \n",
    "    return {'meta':metadat,\n",
    "            'start': begin,\n",
    "            'end': end,\n",
    "            'htstart':htbegin, \n",
    "            'htend': htend} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Large function that will return to you the relevant game events and then a list of tweets (with their times) \n",
    "from the time period that you specify. Currently works by submitting actual times.\n",
    "\n",
    "Inputs:\n",
    "gp1: The first game participant\n",
    "gp2: The second game participant\n",
    "gtp1: Beginning of game time period (20:00 -- Halftime, 35:00 -- After halftime)\n",
    "gtp2: End of game time period\n",
    "\n",
    "Outputs:\n",
    "dt1: Dataframe containing all the game events from that period \n",
    "dt2: Dataframe containing the tweets from that period\n",
    "\"\"\"\n",
    "def databygametime(gp1, gp2, gtp1, gtp2):\n",
    "    \n",
    "    # Get the important metadata and define a useful constant\n",
    "    gmdat = pd.read_csv(\"GameMetadata.csv\")\n",
    "    metadat = gmdat[((gmdat['Team1'] == gp1) | (gmdat['Team1'] == gp2)) & ((gmdat['Team2'] == gp1) | (gmdat['Team2'] == gp2))]\n",
    "    eid = metadat['espn_id'].iloc[0]\n",
    "    begin, end = datetime.strptime(metadat['Start'].iloc[0], '%H:%M'), datetime.strptime(metadat['End'].iloc[0], '%H:%M')\n",
    "    lenhalf = 20*60\n",
    "    htbegin = begin + timedelta(0, (end-begin).seconds/2 - lenhalf/2)\n",
    "    htend = begin + timedelta(0,(end-begin).seconds/2 + lenhalf/2)\n",
    "    \n",
    "    # Grab the initial data by mapping gametimes to parts of the game\n",
    "    fullgamedat = ncaatrendgraph(eid)\n",
    "   \n",
    "    bmin, bsec = gtp1.split(\":\")\n",
    "    emin, esec = gtp2.split(\":\")\n",
    "    btot = int(bmin)*60 + int(bsec)\n",
    "    etot = int(emin)*60 + int(bsec)\n",
    "    maxmin = np.max(fullgamedat['Minutes'])*60\n",
    "    bperc, eperc = (100.0*btot)/maxmin, (100.0*etot)/maxmin\n",
    "    dt1 = fullgamedat[(fullgamedat['PercDone'] >= bperc) & (fullgamedat['PercDone'] <= eperc)]\n",
    "    \n",
    "    # Use the percentages of the game to map gametimes to actual times\n",
    "    acttimes = []\n",
    "    for item in [bperc, eperc]:\n",
    "        hperc = (item*2)/100\n",
    "        if item <= 50.0:\n",
    "            secdelt = round((htbegin-begin).seconds*hperc,0)\n",
    "            acttimes.append(secdelt)\n",
    "        elif item > 50.0:\n",
    "            secdelt = round((end-htend).seconds*(hperc-1.0) + (htend-begin).seconds,0)\n",
    "            acttimes.append(secdelt)\n",
    "    \n",
    "    # Convert the time to usable timedelt objects\n",
    "    tp1, tp2 = begin + timedelta(0,seconds = acttimes[0]), begin + timedelta(0,seconds = acttimes[1])\n",
    "    tp1, tp2 = str(tp1).split(\" \")[1], str(tp2).split(\" \")[1]\n",
    "    \n",
    "    # Get the relevant Tweets over that time period using identical code to before\n",
    "    fname = '../separated/' + metadat['Filename'].iloc[0]\n",
    "    tweets =  pd.read_csv(fname)\n",
    "    tweets['time_chg'] = tweets['time'].apply(lambda x: x.split(' ')[3])\n",
    "    \n",
    "    indices = []\n",
    "    for num in range(0, len(tweets['time_chg'])):\n",
    "        tweetstamp = datetime.strptime(tweets['time_chg'].iloc[num], \"%H:%M:%S\")#  - timedelta(hours=4)\n",
    "        if (tweetstamp > begin) and (tweetstamp < end):\n",
    "            if (tweetstamp > datetime.strptime(tp1, '%H:%M:%S')) and (tweetstamp < datetime.strptime(tp2, '%H:%M:%S')):\n",
    "                indices.append(num)\n",
    "    dt2 = tweets[tweets.index.isin(indices)]\n",
    "    return dt1, dt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samp1, samp2 = databygametime('Virginia', 'Syracuse', '0:00', '40:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samp3, samp4 = databytime('Virginia', 'Syracuse', '18:10', '18:20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tw = samp4.iloc[120]['text']\n",
    "classifier.classify(tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier.classify(\"no doubt best awesome amazing team best product\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samp2['classification'] = samp2['text'].map(lambda t : classifier.classify(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Relevance Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write a quick function that will grab the relevant roster items, given a team id\n",
    "def grabroster(teampage):\n",
    "    url = \"http://\" + teampage.split('team')[0] + 'team/stats' + teampage.split('team')[1]\n",
    "    html = urllib.urlopen(url)\n",
    "    soup = BeautifulSoup(html, \"html.parser\") \n",
    "\n",
    "    names = []\n",
    "    for tr in soup.find_all('table')[0].find_all('tr')[2:9]:\n",
    "        for item in tr.find_all('td')[0:1]:\n",
    "            names.append(str(item.contents[0].contents[0]).split(' ')[1])\n",
    "    return names\n",
    "    \n",
    "# Write a function that will return two lists, each of which contains the last names of the top six players (as measured\n",
    "# by playing time), the name of the associated coach for the team, and the \n",
    "def relvtags(gameid):\n",
    "    gameid = 400873651\n",
    "    url = 'http://espn.go.com/mens-college-basketball/playbyplay?gameId=' + str(gameid)\n",
    "    html = urllib.urlopen(url)\n",
    "    soup = BeautifulSoup(html, \"html.parser\") \n",
    "\n",
    "    # Identify which team is which from the basic page\n",
    "    teams = soup.find_all('div', {'class', \"team-container\"}) \n",
    "\n",
    "    # Find the links to the right pages\n",
    "    teamlinks = [str(\"espn.go.com\" + teams[0].find_all('a')[0]['href']), \n",
    "                 str(\"espn.go.com\" + teams[1].find_all('a')[0]['href'])]\n",
    "\n",
    "    # Grab the correct roster for the team\n",
    "    team1, team2 = grabroster(teamlinks[0]), grabroster(teamlinks[1])\n",
    "    coach1, coach2 = coachdict[int(teamlinks[0].split('/')[-1])], coachdict[int(teamlinks[1].split('/')[-1])]\n",
    "    team1.append(coach1.split(' ')[1])\n",
    "    team2.append(coach2.split(' ')[1])\n",
    "    print team1, team2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the relevant dictionaries for the stuff coming after\n",
    "coachurl = \"http://espn.go.com/mens-college-basketball/story/_/id/14967272/ranking-ncaa-tournament-coaches-players-1-68\"\n",
    "coachhtml = urllib.urlopen(coachurl)\n",
    "coachsoup = BeautifulSoup(coachhtml, \"html.parser\")\n",
    "\n",
    "# Create a useful dictionary for the ids and the coach\n",
    "coachdict = {}\n",
    "for item in coachsoup.find_all('strong'):\n",
    "    name = str(item.contents[0].split('. ')[1][:-2])\n",
    "    \n",
    "    # Get the associated teamid, throwing an exception for Southern (only non-linked team in the database)\n",
    "    if ',' not in name:\n",
    "        gid = int(item.find_all('a')[0]['href'].split('id/')[1].split('/')[0])\n",
    "    else:\n",
    "        gid = 2582\n",
    "        name = name.split(',')[0]\n",
    "    coachdict[gid] = name\n",
    "\n",
    "# Define a dict with tags for each team\n",
    "tagdict = {12: ['#Wildcats', '#Arizona', '#GoCats', '#BearDown', '#AZWildcats', '#watchus', '#APlayersProgram'],\n",
    "           25: ['#Cal', '#LetsDance', '#GoldenBears', '#GoBears', '#CalFamily'],\n",
    "           38: ['#Buffaloes', '#Colorado', '#GoBuffs'],\n",
    "           41: ['#Huskies', '#UConn', '#Connecticut', '#bleedblue', '#GoHuskies', '#UConnbasketball', '#UConnnation'],\n",
    "           43: ['#Bulldogs', '#Yale', '#OneIvy', '#YaleBasketball', '#GoYale'],\n",
    "           62: ['#RainbowWarriors', '#Hawaii', '#HawaiiMBB', '#RoadWarriors', '#GoBows'],\n",
    "           66: ['#Cyclones', '#IowaSt', '#IowaState', '#cyclONEnation'],\n",
    "           84: ['#Hoosiers', '#IU', '#ForIndiana', '#iubb'],\n",
    "           87: ['#NotreDame', '#NotDoneYet', '#FightingIrish'],\n",
    "           96: ['#Kentucky', '#Wildcats', '#Cats', '#BigBlueNation'],\n",
    "           107: ['#HolyCross', '#Crusaders', '#RiseTogether'],\n",
    "           120: ['#WeWill', '#Maryland', '#Terrapins'],\n",
    "           127: ['#Spartans', '#GoGreen', '#GoWhite', '#MSU', '#MichiganState'],\n",
    "           130: ['#Michigan', '#UMich', '#Wolverines', '#Squad100', '#GoBlue'],\n",
    "           150: ['#Duke', '#BlueDevils', '#GoDuke'],\n",
    "           153: ['#UNC', '#TarHeels', '#HeelsLockIn', '#UNCBBall'],\n",
    "           183: ['#Syracuse', '#OrangeCrush', '#CuseMode', '#StLouis'],\n",
    "           201: ['#Oklahoma', '#OU', '#Sooners', '#BuddyBuckets'],\n",
    "           204: ['#Beavers', '#OregonState', '#oregonstatebasketball', '#BeaverNation', '#GoBeavs'],\n",
    "           218: ['#Temple', '#Owls', '#BeatIowa', '#TUMBB'],\n",
    "           221: ['#Pittsburgh', '#Panthers', '#H2P'],\n",
    "           222: ['#Villanova', '#Nova', '#NovaMBB', '#NovaNation', '#LetsMarchNova'],\n",
    "           239: ['#Bears', '#Baylor', '#GoBears'],\n",
    "           245: ['#TexasA&M', '#Aggies', '#Gigem', '#TAMU', '#12thMan', '#AggieHoops'],\n",
    "           251: ['#UT', '#Longhorns', '#HookEm', '#Horns'],\n",
    "           254: ['#Utah', '#Utes', '#goutes', '#Playformore', '#BeatGonzaga'],\n",
    "           258: ['#Virginia', '#Cavaliers', '#Bulldogs', '#UVA', '#GoHoos'],\n",
    "           275: ['#Badgers', '#Wisconsin', '#MakeEmBelieve', '#Fieldof64', '#WisconsinBasketball'],\n",
    "           277: ['#WestVirginia', '#WVU', '#HailWV', '#PressVirginia', '#Mountaineers'],\n",
    "           2031: ['#LittleRocksTeam'],\n",
    "           2046: ['#AustinPeay', '#LetsGoPeay', '#16over1'],\n",
    "           2084: ['#Bison', '#Buffalo', '#UBBulls', '#UBDancing', '#HornsUp'],\n",
    "           2086: ['#Butler', '#Bulldogs', '#GoDawgs'],\n",
    "           2132: ['#Cincinnati', '#Cincy', '#BearCats', '#Dancin6'],\n",
    "           2168: ['#Dayton', '#Flyers', '#TrueTeam', '#GoFlyers'],\n",
    "           2250: ['#Zags', '#Gonzaga', '#GoZags', '#UnitedWeZag', '#zagup'],\n",
    "           2294: ['#Hawkeyes', '#Iowa'],\n",
    "           2305: ['#Jayhawks', '#kubball', '#Kansas', '#RockChalk'],\n",
    "           2390: ['#Hurricanes', '#Miami', '#BeatBuffalo', '#GoCanes'],\n",
    "           2393: ['#MiddleTennesseeState', '#MTSU', '#BlueRaiders', '#DancingRaiders'],\n",
    "           2427: ['#OurTownOurTeam', '#Bulldogs', '#Cinderella', '#UNCAsheville', '#Asheville'],\n",
    "           2460: ['#NorthernIowa', '#Panthers', '#PantherNation', '#UNI', '#UNIFight'],\n",
    "           2483: ['#Oregon', '#Ducks', '#GoDucks'],\n",
    "           2507: ['#Friars', '#PCBB', '#gofriars', '#pcbb', '#dunnions'],\n",
    "           2550: ['#SetonHall', '#HALLin', '#shbb', '#SHUnited'],\n",
    "           2571: ['#SDSU', '#Jackrabbits', '#MarchTogether'],\n",
    "           2603: ['#StJoes', '#Hawks', '#StJosephs', '#THWND'],\n",
    "           2617: ['#Lumberjacks', '#SFA', '#AxeEm'],\n",
    "           2670: ['#Rams', '#LetsGoVCU', '#VCU'],\n",
    "           2692: ['#Wildcats', '#WeberState', '#WeAreWeber', '#BigSkyMBB'],\n",
    "           2724: ['#Wildcats', '#Shockers', '#Shocktheworld', '#WichitaSt'],\n",
    "           2739: ['#RP40', '#HLMBB', '#ContinueTheRise', '#GreenBay'],\n",
    "           2752: ['#Xavier', '#Musketeers', '#LetsGoX', '#LetsMarch'],\n",
    "           2934: ['#CSUBelieve', '#AllRunners', '#CSUB']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write a quick function that will grab the relevant roster items, given a team id\n",
    "def grabroster(teampage):\n",
    "    url = \"http://\" + teampage.split('team')[0] + 'team/stats' + teampage.split('team')[1]\n",
    "    html = urllib.urlopen(url)\n",
    "    soup = BeautifulSoup(html, \"html.parser\") \n",
    "\n",
    "    names = []\n",
    "    for tr in soup.find_all('table')[0].find_all('tr')[2:9]:\n",
    "        for item in tr.find_all('td')[0:1]:\n",
    "            names.append(str(item.contents[0].contents[0]).split(' ')[1])\n",
    "    return names\n",
    "    \n",
    "# Write a function that will return two lists, each of which contains the last names of the top six players (as measured\n",
    "# by playing time), the name of the associated coach for the team, and the \n",
    "def relvtags(gameid):\n",
    "    url = 'http://espn.go.com/mens-college-basketball/playbyplay?gameId=' + str(gameid)\n",
    "    html = urllib.urlopen(url)\n",
    "    soup = BeautifulSoup(html, \"html.parser\") \n",
    "\n",
    "    # Identify which team is which from the basic page\n",
    "    teams = soup.find_all('div', {'class', \"team-container\"}) \n",
    "\n",
    "    # Find the links to the right pages\n",
    "    teamlinks = [str(\"espn.go.com\" + teams[0].find_all('a')[0]['href']), \n",
    "                 str(\"espn.go.com\" + teams[1].find_all('a')[0]['href'])]\n",
    "\n",
    "    # Grab the correct roster for the team\n",
    "    x = []\n",
    "    for item in teamlinks:\n",
    "        team = grabroster(item)\n",
    "        gid = int(item.split('/')[-1])\n",
    "        team.append(coachdict[gid].split(' ')[1])\n",
    "        for tag in tagdict[gid]:\n",
    "            team.append(tag)\n",
    "            team.append(tag.split('#')[1])\n",
    "        x.append(team)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Go through and classify whether a text is in the tweet\n",
    "def textcheck(tweet, array):\n",
    "    relvs = []\n",
    "    for tagset in array:\n",
    "        counter = 0\n",
    "        for word in tagset:\n",
    "            if word in tweet:\n",
    "                counter += 1\n",
    "        # Check if anything showed up\n",
    "        if counter > 0:\n",
    "            relvs.append(1)\n",
    "            relvs.append(counter)\n",
    "        else:\n",
    "            relvs.append(0)\n",
    "        relvs.append(0)\n",
    "    return relvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Given relevant keywords for the 2 teams, add what keywords appear for each.\n",
    "\"\"\"\n",
    "def addrelevance(df, wordset):\n",
    "    def textchecksingle(tweet, wset):\n",
    "        twdict = {w:True for w in tweet.split()}\n",
    "        return [w for w in wset if w in twdict]\n",
    "    \n",
    "    df['Team1RelWords'] = df['text'].map(lambda t : textchecksingle(t, tags[0]))\n",
    "    df['Team2RelWords'] = df['text'].map(lambda t : textchecksingle(t,tags[1]))\n",
    "    \n",
    "    t1len = len(tags[0])\n",
    "    t2len = len(tags[1])\n",
    "    \n",
    "    df['Team1RelScore'] = df['Team1RelWords'].map(lambda t : float(len(t))/t1len)\n",
    "    df['Team2RelScore'] = df['Team2RelWords'].map(lambda t : float(len(t))/t2len)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tags = relvtags(400873156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samp2 = addrelevance(samp2,tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eastern = pytz.timezone('US/Eastern')\n",
    "samp2.index = samp2['time'].map(lambda x : parser.parse(x).astimezone(eastern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samp2['Team1RelScore'] = samp2['Team1RelScore'] * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classlam(t):\n",
    "    cla = classifier.classify(t)\n",
    "    if cla== 'neutral' or cla == 'irrelevant':\n",
    "        return 0\n",
    "    elif cla == 'positive':\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samp2['Sentiments'] = samp2['text'].map(classlam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countts = samp2['text'].groupby([samp2.index.hour, samp2.index.minute]).count().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tweeteventplotandregression(team1, team2):\n",
    "    gamemeta = getgamemeta(team1, team2)\n",
    "    events, tweets = databygametime(team1, team2, '0:00', '40:00')    \n",
    "    eastern = pytz.timezone('US/Eastern')\n",
    "    tweets.index = tweets['time'].map(lambda x : parser.parse(x)) #  .astimezone(eastern))\n",
    "    \n",
    "    events['Delta'] = events.apply(lambda r : timedelta(minutes=r['Minutes']) +\n",
    "                                            timedelta(seconds = r['Seconds']), axis=1)\n",
    "    events['FirstHalf'] = events.apply(lambda r : r['Delta'] < timedelta(minutes=20), axis = 1)\n",
    "    \n",
    "    def addclock(row):\n",
    "        if row['FirstHalf']:\n",
    "            return row['Delta'] + gamemeta['start']\n",
    "        else:\n",
    "            return row['Delta'] + gamemeta['htend']\n",
    "    \n",
    "    events['WallClockTime'] = events.apply(addclock, axis=1)\n",
    "    events.index = events['WallClockTime']\n",
    "    \n",
    "#     evtotal = sum(events['Event'].groupby([events.index.hour, events.index.minute]).count())\n",
    "#     twtotal = sum(tweets['text'].groupby([tweets.index.hour,tweets.index.minute]).count())\n",
    "       \n",
    "    # group by hour and minute\n",
    "    eventts = events['Event'].groupby([events.index.hour, events.index.minute]).count() # /evtotal\n",
    "    tweetts = tweets['text'].groupby([tweets.index.hour,tweets.index.minute]).count() # /twtotal\n",
    "    \n",
    "    # convert groupings to dataframe\n",
    "    eventtsdf = eventts.to_frame()\n",
    "    tweettsdf = tweetts.to_frame()\n",
    "    \n",
    "    jointdf = tweettsdf.join(eventtsdf).fillna(0)\n",
    "    \n",
    "    # make plots\n",
    "    jointdf['text'].plot()\n",
    "    jointdf['Event'].plot()\n",
    "    \n",
    "    # run a basic regression\n",
    "    \n",
    "    reg = slope, intercept, r_value, p_value, slope_std_error = stats.linregress(jointdf['Event'], jointdf['text'])\n",
    "    \n",
    "    preds = intercept + slope*jointdf['Event'].values\n",
    "    plt.plot(preds)\n",
    "    plt.title('Tweet and Event Volumes')\n",
    "    plt.xlabel(\"Minute in Game\")\n",
    "    plt.ylabel('Volume (per minute)')\n",
    "    plt.legend(['Tweets', 'Game Events'])\n",
    "    plt.savefig(team1+team2+'tsvolume.pdf')\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.scatter(jointdf['Event'], jointdf['text'])\n",
    "    plt.plot(jointdf['Event'], preds)\n",
    "    \n",
    "    return (reg)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def marginplot(team1, team2):\n",
    "    gamemeta = getgamemeta(team1, team2)\n",
    "    events, tweets = databygametime(team1, team2, '0:00', '40:00')    \n",
    "    eastern = pytz.timezone('US/Eastern')\n",
    "    tweets.index = tweets['time'].map(lambda x : parser.parse(x)) #  .astimezone(eastern))\n",
    "    \n",
    "    events['Delta'] = events.apply(lambda r : timedelta(minutes=r['Minutes']) +\n",
    "                                                timedelta(seconds = r['Seconds']), axis=1)\n",
    "    events['FirstHalf'] = events.apply(lambda r : r['Delta'] < timedelta(minutes=20), axis = 1)\n",
    "\n",
    "    def addclock(row):\n",
    "        if row['FirstHalf']:\n",
    "            return row['Delta'] + gamemeta['start']\n",
    "        else:\n",
    "            return row['Delta'] + gamemeta['htend']\n",
    "\n",
    "    events['WallClockTime'] = events.apply(addclock, axis=1)\n",
    "    events.index = events['WallClockTime']\n",
    "    \n",
    "    events['Margin'] = events['North Carolina'] - events['Notre Dame']\n",
    "    events['MarginChange'] = abs(events['Margin'] - events['Margin'].shift())\n",
    "    \n",
    "    \n",
    "    marginchangets = events['MarginChange'].groupby([events.index.hour, events.index.minute]).mean()\n",
    "    tweetts = tweets['text'].groupby([tweets.index.hour,tweets.index.minute]).count() # /twtotal\n",
    "\n",
    "    eventtsdf = marginchangets.to_frame()\n",
    "    tweettsdf = tweetts.to_frame()\n",
    "    \n",
    "    jointdf = tweettsdf.join(eventtsdf).fillna(0)\n",
    "    \n",
    "    # make plots\n",
    "    normtext = (jointdf['text'] - jointdf['text'].mean())/jointdf['text'].std()\n",
    "    normmargin = (jointdf['MarginChange'] - jointdf['MarginChange'].mean())/jointdf['MarginChange'].std()\n",
    "    (normtext).plot()\n",
    "    (normmargin).plot()\n",
    "    \n",
    "    # run a basic regression\n",
    "    \n",
    "    reg = slope, intercept, r_value, p_value, slope_std_error = stats.linregress(normtext[jointdf['MarginChange'] != 0], normmargin[jointdf['MarginChange'] != 0],)\n",
    "    \n",
    "    preds = intercept + slope*jointdf['MarginChange'].values\n",
    "    \n",
    "    plt.title('Normalized Volume and Game Margin')\n",
    "    plt.xlabel(\"Minute in Game, {0} vs {1}\".format(team1, team2))\n",
    "    plt.ylabel('z-score')\n",
    "    plt.legend(['Tweets', 'Change in Margin'])\n",
    "    plt.savefig(team1.replace(' ', '').lower()+team2.replace(' ', '').lower()+'marginzscoreplot.pdf')\n",
    "    \n",
    "    return (reg)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "marginplot('North Carolina', 'Notre Dame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reg = tweeteventplotandregression('UNC','Notre Dame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamemeta = getgamemeta('Syracuse', 'Virginia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gamemeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gamemeta['start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print gamemeta['htstart']\n",
    "print gamemeta['htend']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samp1['Delta'] = samp1.apply(lambda r : timedelta(minutes=r['Minutes']) + timedelta(seconds = r['Seconds']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samp1['FirstHalf'] = samp1.apply(lambda r : r['Delta'] < timedelta(minutes=20), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addclock(row):\n",
    "    if row['FirstHalf']:\n",
    "        return row['Delta'] + gamemeta['start']\n",
    "    else:\n",
    "        return row['Delta'] + gamemeta['htend']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samp1['WallClockTime'] = samp1.apply(addclock, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samp1.index = samp1['WallClockTime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evtotal = sum(samp1['Event'].groupby([samp1.index.hour, samp1.index.minute]).count())\n",
    "twtotal = sum(samp2['text'].groupby([samp2.index.hour,samp2.index.minute]).count())\n",
    "eventts = samp1['Event'].groupby([samp1.index.hour, samp1.index.minute]).count()/evtotal\n",
    "tweetts = samp2['text'].groupby([samp2.index.hour,samp2.index.minute]).count()/twtotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eventtsdf = eventts.to_frame()\n",
    "tweettsdf = tweetts.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jointdf = tweettsdf.join(eventtsdf).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats.linregress(jointdf['Event'], jointdf['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jointdf['text'].plot()\n",
    "jointdf['Event'].plot()\n",
    "plt.savefig('tweetandevent.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
